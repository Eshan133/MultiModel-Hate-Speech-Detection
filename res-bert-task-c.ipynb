{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12431539,"sourceType":"datasetVersion","datasetId":7841536}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Resnet and Bert","metadata":{}},{"cell_type":"code","source":"# 1. Required Imports\nimport os, re, json\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom transformers import BertTokenizer, BertModel, get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:18.001299Z","iopub.execute_input":"2025-07-20T06:30:18.001849Z","iopub.status.idle":"2025-07-20T06:30:43.050982Z","shell.execute_reply.started":"2025-07-20T06:30:18.001830Z","shell.execute_reply":"2025-07-20T06:30:43.050426Z"}},"outputs":[{"name":"stderr","text":"2025-07-20 06:30:32.228845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752993032.422197      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752993032.477161      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nprint([f for f in os.listdir() if 'torchvision' in f])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:43.051659Z","iopub.execute_input":"2025-07-20T06:30:43.052100Z","iopub.status.idle":"2025-07-20T06:30:43.056450Z","shell.execute_reply.started":"2025-07-20T06:30:43.052074Z","shell.execute_reply":"2025-07-20T06:30:43.055785Z"}},"outputs":[{"name":"stdout","text":"[]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 2. Label Mapping\nLABEL_MAP = {'Neutral': 0, 'Support': 1, 'Oppose': 2}\nINVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\nroot = \"/kaggle/input/subtask3-comp2025-multimodel/\"\nrandom.seed(42)\ntorch.manual_seed(42)\n\n# 3. Clean text function\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|@\\w+|#\\w+|[^\\w\\s]\", \"\", text)\n    return re.sub(r\"\\s+\", \" \", text).strip()\n\n# 4. Load DataFrames\ndef getIndexAndPath(folder):\n    paths = []\n    for filename in os.listdir(folder):\n        if filename.lower().endswith((\".png\")):\n            paths.append({\n                \"index\": filename,\n                \"image_path\": os.path.join(folder, filename)\n            })\n    return paths\n\nrecords = []\n# Train DF\nrecords = []\nfor label_name in os.listdir(os.path.join(root, \"train/Subtask C Train\")):\n    label_folder = os.path.join(os.path.join(root, \"train/Subtask C Train\"), label_name)\n    if not os.path.isdir(label_folder): continue\n    label_id = LABEL_MAP[label_name]\n    records += getIndexAndPath(label_folder)\ndf_images = pd.DataFrame(records)\ndf_ocr = pd.read_csv(os.path.join(root, \"train/STask_C_train.csv\"))\ndf_train = pd.merge(df_images, df_ocr, on=\"index\", how=\"left\")\n# Test DF\ndf_images = pd.DataFrame(getIndexAndPath(os.path.join(root, \"test/STask_C_test_img\")))\ndf_ocr = pd.read_csv(os.path.join(root, \"test/STask-C(index,text)test.csv\"))\ndf_test = pd.merge(df_images, df_ocr, on=\"index\", how=\"left\")\n# Eval DF\ndf_images = pd.DataFrame(getIndexAndPath(os.path.join(root, \"eval/STask_C_val_img\")))\ndf_ocr = pd.read_csv(os.path.join(root, \"eval/STask-C(index,text)val.csv\"))\ndf_labels = pd.read_csv(os.path.join(root, \"eval/STask-C(index,label)val.csv\"))\ndf_eval = pd.merge(df_images, df_ocr, on=\"index\", how=\"left\")\ndf_val = pd.merge(df_eval, df_labels, on=\"index\", how=\"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:43.057916Z","iopub.execute_input":"2025-07-20T06:30:43.058106Z","iopub.status.idle":"2025-07-20T06:30:43.267268Z","shell.execute_reply.started":"2025-07-20T06:30:43.058091Z","shell.execute_reply":"2025-07-20T06:30:43.266776Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 5. Clean Text\nfor df in [df_train, df_val, df_test]:\n    df[\"text\"] = df[\"text\"].fillna(\"[NO TEXT]\").apply(clean_text)\n\n# 6. Class Weights\nclass_weights = compute_class_weight('balanced', classes=list(LABEL_MAP.values()), y=df_train['label'])\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\n\n# 7. Tokenizer and Transform\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nimg_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:43.267943Z","iopub.execute_input":"2025-07-20T06:30:43.268228Z","iopub.status.idle":"2025-07-20T06:30:44.093565Z","shell.execute_reply.started":"2025-07-20T06:30:43.268210Z","shell.execute_reply":"2025-07-20T06:30:44.092862Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb93469dc14e42028873b9f42d70544e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c875d5f430d94706b5d5ba523df0609b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b807cfd75498476da43cea1113e9b9dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7178339e4a43abb714e65e1cce8088"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# 8. Dataset\nclass MemeCLIPDataset(Dataset):\n    def __init__(self, df, tokenizer, transform, is_train=True):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.is_train = is_train\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row.image_path).convert(\"RGB\")\n        ocr_text = row.text\n        text_encoding = self.tokenizer(ocr_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n        image_tensor = self.transform(image)\n\n        sample = {\n            \"input_ids\": text_encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": text_encoding[\"attention_mask\"].squeeze(0),\n            \"pixel_values\": image_tensor\n        }\n\n        if self.is_train:\n            sample[\"label\"] = int(row.label)\n        else:\n            sample[\"index\"] = row[\"index\"]\n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:44.094318Z","iopub.execute_input":"2025-07-20T06:30:44.094573Z","iopub.status.idle":"2025-07-20T06:30:44.100389Z","shell.execute_reply.started":"2025-07-20T06:30:44.094546Z","shell.execute_reply":"2025-07-20T06:30:44.099774Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 9. Collate Function\ndef collate_fn(batch):\n    input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n    attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n    if \"label\" in batch[0]:\n        labels = torch.tensor([x[\"label\"] for x in batch])\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pixel_values\": pixel_values, \"labels\": labels}\n    else:\n        indices = [x[\"index\"] for x in batch]\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pixel_values\": pixel_values, \"index\": indices}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:44.101107Z","iopub.execute_input":"2025-07-20T06:30:44.101335Z","iopub.status.idle":"2025-07-20T06:30:44.112928Z","shell.execute_reply.started":"2025-07-20T06:30:44.101314Z","shell.execute_reply":"2025-07-20T06:30:44.112418Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 10. MemeCLIP Model\nclass MemeCLIP(nn.Module):\n    def __init__(self, text_model, image_model, num_classes=3):\n        super().__init__()\n        self.text_encoder = text_model\n        self.image_encoder = image_model\n        self.image_proj = nn.Linear(2048, 768)\n        self.dropout = nn.Dropout(0.5)\n        self.classifier = nn.Sequential(\n            nn.Linear(768*2, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, pixel_values, labels=None):\n        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n        text_emb = text_out.last_hidden_state[:, 0, :]\n        img_feat = self.image_encoder(pixel_values)\n        img_emb = self.image_proj(img_feat)\n        fused = torch.cat([text_emb, img_emb], dim=1)\n        logits = self.classifier(self.dropout(fused))\n        if labels is not None:\n            loss = nn.CrossEntropyLoss(weight=class_weights.to(logits.device))(logits, labels)\n            return loss, logits\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:44.113688Z","iopub.execute_input":"2025-07-20T06:30:44.114152Z","iopub.status.idle":"2025-07-20T06:30:44.126960Z","shell.execute_reply.started":"2025-07-20T06:30:44.114131Z","shell.execute_reply":"2025-07-20T06:30:44.126435Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 11. Prepare DataLoaders\ntrain_dataset = MemeCLIPDataset(df_train, tokenizer, img_transform)\nval_dataset = MemeCLIPDataset(df_val, tokenizer, img_transform)\ntest_dataset = MemeCLIPDataset(df_test, tokenizer, img_transform, is_train=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n\n# 12. Load Models and Init\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nresnet_model = models.resnet50(pretrained=True)\nresnet_model.fc = nn.Identity()\nmodel = MemeCLIP(bert_model, resnet_model).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\noptimizer = AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_loader) * 10\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:44.127679Z","iopub.execute_input":"2025-07-20T06:30:44.127919Z","iopub.status.idle":"2025-07-20T06:30:48.318333Z","shell.execute_reply.started":"2025-07-20T06:30:44.127903Z","shell.execute_reply":"2025-07-20T06:30:48.317581Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b153fa328fa4bd797816791155897f8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 112MB/s] \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 13. Train & Eval Functions\ndef train_one_epoch(model, loader, optimizer, scheduler, device):\n    model.train()\n    total_loss, preds, labels = 0, [], []\n    for batch in tqdm(loader, desc=\"Train\"):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        loss, logits = model(**batch)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n        preds += logits.argmax(1).cpu().tolist()\n        labels += batch[\"labels\"].cpu().tolist()\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average=\"macro\")\n    return total_loss/len(loader), acc, f1\n\ndef evaluate(model, loader, device):\n    model.eval()\n    total_loss, preds, labels = 0, [], []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Eval\"):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            loss, logits = model(**batch)\n            total_loss += loss.item()\n            preds += logits.argmax(1).cpu().tolist()\n            labels += batch[\"labels\"].cpu().tolist()\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average=\"macro\")\n    return total_loss/len(loader), acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:48.321085Z","iopub.execute_input":"2025-07-20T06:30:48.321458Z","iopub.status.idle":"2025-07-20T06:30:48.328441Z","shell.execute_reply.started":"2025-07-20T06:30:48.321439Z","shell.execute_reply":"2025-07-20T06:30:48.327901Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 14. Training Loop with Early Stopping\nbest_f1 = 0\npatience = 3\ncounter = 0\nfor epoch in range(10):\n    print(f\"\\nEpoch {epoch+1}\")\n    tr_loss, tr_acc, tr_f1 = train_one_epoch(model, train_loader, optimizer, scheduler, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    val_loss, val_acc, val_f1 = evaluate(model, val_loader, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    print(f\"Train Loss: {tr_loss:.4f} | Acc: {tr_acc:.4f} | F1: {tr_f1:.4f}\")\n    print(f\"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        counter = 0\n        torch.save(model.state_dict(), \"best_model.pt\")\n        print(\"✅ Saved best model\")\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"⏹️ Early stopping\")\n            break","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-20T06:30:48.329180Z","iopub.execute_input":"2025-07-20T06:30:48.329424Z","iopub.status.idle":"2025-07-20T07:16:34.661754Z","shell.execute_reply.started":"2025-07-20T06:30:48.329404Z","shell.execute_reply":"2025-07-20T07:16:34.660969Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 254/254 [05:05<00:00,  1.20s/it]\nEval: 100%|██████████| 32/32 [00:24<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0865 | Acc: 0.3760 | F1: 0.3761\nVal   Loss: 1.0203 | Acc: 0.5138 | F1: 0.4735\n✅ Saved best model\n\nEpoch 2\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 254/254 [04:38<00:00,  1.10s/it]\nEval: 100%|██████████| 32/32 [00:21<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9670 | Acc: 0.5326 | F1: 0.5286\nVal   Loss: 0.9208 | Acc: 0.5771 | F1: 0.5771\n✅ Saved best model\n\nEpoch 3\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 254/254 [04:39<00:00,  1.10s/it]\nEval: 100%|██████████| 32/32 [00:21<00:00,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7177 | Acc: 0.7037 | F1: 0.6999\nVal   Loss: 0.9328 | Acc: 0.5909 | F1: 0.5884\n✅ Saved best model\n\nEpoch 4\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 254/254 [04:40<00:00,  1.10s/it]\nEval: 100%|██████████| 32/32 [00:21<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3049 | Acc: 0.8995 | F1: 0.8977\nVal   Loss: 1.1956 | Acc: 0.5968 | F1: 0.5958\n✅ Saved best model\n\nEpoch 5\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 254/254 [04:38<00:00,  1.10s/it]\nEval: 100%|██████████| 32/32 [00:21<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1115 | Acc: 0.9709 | F1: 0.9705\nVal   Loss: 1.3881 | Acc: 0.6008 | F1: 0.5990\n✅ Saved best model\n\nEpoch 6\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 254/254 [04:38<00:00,  1.10s/it]\nEval: 100%|██████████| 32/32 [00:21<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0481 | Acc: 0.9894 | F1: 0.9892\nVal   Loss: 1.5422 | Acc: 0.6126 | F1: 0.6042\n✅ Saved best model\n\nEpoch 7\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 254/254 [04:39<00:00,  1.10s/it]\nEval: 100%|██████████| 32/32 [00:21<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0271 | Acc: 0.9943 | F1: 0.9943\nVal   Loss: 1.6309 | Acc: 0.5909 | F1: 0.5878\n\nEpoch 8\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 254/254 [04:42<00:00,  1.11s/it]\nEval: 100%|██████████| 32/32 [00:21<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0227 | Acc: 0.9956 | F1: 0.9954\nVal   Loss: 1.6920 | Acc: 0.5988 | F1: 0.5947\n\nEpoch 9\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 254/254 [04:39<00:00,  1.10s/it]\nEval: 100%|██████████| 32/32 [00:21<00:00,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0175 | Acc: 0.9960 | F1: 0.9961\nVal   Loss: 1.7113 | Acc: 0.6067 | F1: 0.6007\n⏹️ Early stopping\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# # 15. Inference and Export to JSON\n# def predict_and_export(model, loader, output_file=\"submission.json\"):\n#     model.eval()\n#     predictions = []\n#     with torch.no_grad():\n#         for batch in tqdm(loader, desc=\"Predicting\"):\n#             indices = batch.pop(\"index\")\n#             batch = {k: v.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n#             logits = model(**batch)\n#             preds = torch.argmax(logits, dim=1).cpu().tolist()\n#             for idx, label in zip(indices, preds):\n#                 predictions.append({\"index\": idx, \"prediction\": INVERSE_LABEL_MAP[label]})\n#     with open(output_file, \"w\") as f:\n#         json.dump(predictions, f, indent=2)\n#     print(f\"✅ Predictions saved to {output_file}\")\n\n# # Load Best Model and Predict\n# model.load_state_dict(torch.load(\"best_model.pt\"))\n# predict_and_export(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:16:34.662496Z","iopub.execute_input":"2025-07-20T07:16:34.662714Z","iopub.status.idle":"2025-07-20T07:16:34.666656Z","shell.execute_reply.started":"2025-07-20T07:16:34.662699Z","shell.execute_reply":"2025-07-20T07:16:34.665982Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def predict_and_export(model, loader, output_file=\"submission.json\"):\n    model.eval()\n    device = next(model.parameters()).device\n    predictions = []\n\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Predicting\"):\n            indices = batch.pop(\"index\")\n            indices = [str(i) for i in indices]  # ensure index is string like '20568.png'\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            logits = model(**batch)\n            if isinstance(logits, tuple):\n                logits = logits[1]\n            preds = torch.argmax(logits, dim=1).cpu().tolist()\n\n            for idx, label in zip(indices, preds):\n                predictions.append({\"index\": idx, \"prediction\": label})\n    predictions = sorted(predictions, key=lambda x: x[\"index\"])\n    # Write each prediction as one JSON object per line\n    with open(output_file, \"w\") as f:\n        for item in predictions:\n            json.dump(item, f)\n            f.write(\"\\n\")\n\n    print(f\"✅ Predictions saved to {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:16:34.667223Z","iopub.execute_input":"2025-07-20T07:16:34.667409Z","iopub.status.idle":"2025-07-20T07:16:34.681033Z","shell.execute_reply.started":"2025-07-20T07:16:34.667394Z","shell.execute_reply":"2025-07-20T07:16:34.680444Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Load best weights\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:16:34.681674Z","iopub.execute_input":"2025-07-20T07:16:34.682310Z","iopub.status.idle":"2025-07-20T07:16:35.181080Z","shell.execute_reply.started":"2025-07-20T07:16:34.682285Z","shell.execute_reply":"2025-07-20T07:16:35.180452Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Export predictions to file\npredict_and_export(model, test_loader, output_file=\"submission.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:16:35.181732Z","iopub.execute_input":"2025-07-20T07:16:35.181917Z","iopub.status.idle":"2025-07-20T07:16:59.675869Z","shell.execute_reply.started":"2025-07-20T07:16:35.181904Z","shell.execute_reply":"2025-07-20T07:16:59.674288Z"}},"outputs":[{"name":"stderr","text":"Predicting: 100%|██████████| 32/32 [00:24<00:00,  1.31it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Predictions saved to submission.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# 2. Clip Model","metadata":{}},{"cell_type":"code","source":"# =====================\n# Upgraded MemeCLIP Stance Classification Model\n# =====================\n\nimport os, re, json, random\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, ColorJitter, ToTensor, Normalize\nfrom transformers import CLIPProcessor, CLIPModel, get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:16:59.676832Z","iopub.execute_input":"2025-07-20T07:16:59.677130Z","iopub.status.idle":"2025-07-20T07:16:59.743442Z","shell.execute_reply.started":"2025-07-20T07:16:59.677109Z","shell.execute_reply":"2025-07-20T07:16:59.742892Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# =====================\n# Constants and Paths\n# =====================\nLABEL_MAP = {'Neutral': 0, 'Support': 1, 'Oppose': 2}\nINVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\nroot = \"/kaggle/input/subtask3-comp2025-multimodel/\"\nrandom.seed(42)\ntorch.manual_seed(42)\n# =====================\n# Text Cleaning\n# =====================\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|@\\w+|#\\w+|[^\\w\\s]\", \"\", text)\n    return re.sub(r\"\\s+\", \" \", text).strip()\n\n# =====================\n# Data Loading\n# =====================\ndef getIndexAndPath(folder):\n    paths = []\n    for filename in os.listdir(folder):\n        if filename.lower().endswith(\".png\"):\n            paths.append({\"index\": filename, \"image_path\": os.path.join(folder, filename)})\n    return paths\n\n# Load DataFrames\n# Train DF\nrecords = []\nfor label_name in os.listdir(os.path.join(root, \"train/Subtask C Train\")):\n    label_folder = os.path.join(os.path.join(root, \"train/Subtask C Train\"), label_name)\n    if not os.path.isdir(label_folder): continue\n    label_id = LABEL_MAP[label_name]\n    records += getIndexAndPath(label_folder)\ndf_images = pd.DataFrame(records)\ndf_ocr = pd.read_csv(os.path.join(root, \"train/STask_C_train.csv\"))\ndf_train = pd.merge(df_images, df_ocr, on=\"index\", how=\"left\")\n# Test DF\ndf_images = pd.DataFrame(getIndexAndPath(os.path.join(root, \"test/STask_C_test_img\")))\ndf_ocr = pd.read_csv(os.path.join(root, \"test/STask-C(index,text)test.csv\"))\ndf_test = pd.merge(df_images, df_ocr, on=\"index\", how=\"left\")\n# Eval DF\ndf_images = pd.DataFrame(getIndexAndPath(os.path.join(root, \"eval/STask_C_val_img\")))\ndf_ocr = pd.read_csv(os.path.join(root, \"eval/STask-C(index,text)val.csv\"))\ndf_labels = pd.read_csv(os.path.join(root, \"eval/STask-C(index,label)val.csv\"))\ndf_eval = pd.merge(df_images, df_ocr, on=\"index\", how=\"left\")\ndf_val = pd.merge(df_eval, df_labels, on=\"index\", how=\"left\")\n# Clean text\nfor df in [df_train, df_val, df_test]:\n    df[\"text\"] = df[\"text\"].fillna(\"[NO TEXT]\").apply(clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:16:59.744055Z","iopub.execute_input":"2025-07-20T07:16:59.744227Z","iopub.status.idle":"2025-07-20T07:16:59.882862Z","shell.execute_reply.started":"2025-07-20T07:16:59.744213Z","shell.execute_reply":"2025-07-20T07:16:59.882330Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:16:59.883477Z","iopub.execute_input":"2025-07-20T07:16:59.883675Z","iopub.status.idle":"2025-07-20T07:16:59.900425Z","shell.execute_reply.started":"2025-07-20T07:16:59.883660Z","shell.execute_reply":"2025-07-20T07:16:59.899881Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"         index                                         image_path  \\\n0    44033.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n1    49075.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n2    46634.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n3    48243.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n4    44911.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n..         ...                                                ...   \n501  46409.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n502  47779.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n503  42391.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n504  49704.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n505  43164.png  /kaggle/input/subtask3-comp2025-multimodel/eva...   \n\n                                                  text  label  \n0    anonymous 08 13 19 tue 01 30 00 no 53878455 53...      0  \n1    what if i told you that today some children go...      1  \n2    alejandra caraballo squeer oshae sibley a gay ...      1  \n3    she has insecurities when compared to other pr...      0  \n4                this is a femboy this is a trans girl      1  \n..                                                 ...    ...  \n501                      lmao guys i said i hated figs      2  \n502  mom dad im gay lgbt youths ju dont worry we su...      0  \n503  disastrous f gender reveal name reveal click r...      0  \n504  i wear oversized hoodies exclusively stripy th...      1  \n505  straight men 300 years ago straight men today ...      0  \n\n[506 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>image_path</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44033.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>anonymous 08 13 19 tue 01 30 00 no 53878455 53...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49075.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>what if i told you that today some children go...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>46634.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>alejandra caraballo squeer oshae sibley a gay ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48243.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>she has insecurities when compared to other pr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>44911.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>this is a femboy this is a trans girl</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>46409.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>lmao guys i said i hated figs</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>47779.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>mom dad im gay lgbt youths ju dont worry we su...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>42391.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>disastrous f gender reveal name reveal click r...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>49704.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>i wear oversized hoodies exclusively stripy th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>43164.png</td>\n      <td>/kaggle/input/subtask3-comp2025-multimodel/eva...</td>\n      <td>straight men 300 years ago straight men today ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>506 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# =====================\n# Compute Class Weights\n# =====================\nclass_weights = compute_class_weight('balanced', classes=list(LABEL_MAP.values()), y=df_train['label'])\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\n\n# =====================\n# Transforms and Processor\n# =====================\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:16:59.901160Z","iopub.execute_input":"2025-07-20T07:16:59.901388Z","iopub.status.idle":"2025-07-20T07:17:05.304264Z","shell.execute_reply.started":"2025-07-20T07:16:59.901360Z","shell.execute_reply":"2025-07-20T07:17:05.303451Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74420b89e89d46a4884c622ecac28631"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69cda26db2424003bd74632bbd043ac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"948f556463444974ad27f0cfdf9c59bd"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9913124da5bd4c01804663ff47a2e505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44194b91a4b648dd8e129ec426c49f33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd9a7e0857554e0eb5bdfe0a87af4b91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba5f8a45f404b2ca0f35cf192adadb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c98b56d1614435487219a8b1ac8e6cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4e15d7c2a94c3da997c0bdb91a5f73"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# =====================\n# Transforms\n# =====================\ntrain_transform = Compose([\n    RandomResizedCrop(224, scale=(0.8, 1.0)),\n    RandomHorizontalFlip(),\n    ColorJitter(0.2, 0.2, 0.2, 0.1),\n    ToTensor(),\n    Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n])\n\nval_transform = Compose([\n    ToTensor(),\n    Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:17:05.305154Z","iopub.execute_input":"2025-07-20T07:17:05.305434Z","iopub.status.idle":"2025-07-20T07:17:05.310620Z","shell.execute_reply.started":"2025-07-20T07:17:05.305410Z","shell.execute_reply":"2025-07-20T07:17:05.309696Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# =====================\n# Dataset and Collate\n# =====================\nclass MemeDataset(Dataset):\n    def __init__(self, df, transform=None, is_train=True):\n        self.df = df\n        self.is_train = is_train\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row.image_path).convert(\"RGB\")\n        if self.transform: image = self.transform(image)\n        text = row.text\n        sample = clip_processor(\n            text=[text], images=image,\n            return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128\n        )\n        sample = {k: v.squeeze(0) for k, v in sample.items()}\n        if self.is_train:\n            sample[\"label\"] = int(row.label)\n        else:\n            sample[\"index\"] = row[\"index\"]\n        return sample\n\ndef collate_fn(batch):\n    input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n    attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n    if \"label\" in batch[0]:\n        labels = torch.tensor([x[\"label\"] for x in batch])\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pixel_values\": pixel_values, \"labels\": labels}\n    else:\n        indices = [x[\"index\"] for x in batch]\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pixel_values\": pixel_values, \"index\": indices}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:17:05.311585Z","iopub.execute_input":"2025-07-20T07:17:05.311807Z","iopub.status.idle":"2025-07-20T07:17:05.457037Z","shell.execute_reply.started":"2025-07-20T07:17:05.311793Z","shell.execute_reply":"2025-07-20T07:17:05.456161Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# =====================\n# Focal Loss\n# =====================\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2, weight=None):\n        super().__init__()\n        self.gamma = gamma\n        self.ce = nn.CrossEntropyLoss(weight=weight)\n\n    def forward(self, input, target):\n        logp = self.ce(input, target)\n        p = torch.exp(-logp)\n        return ((1 - p) ** self.gamma * logp).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:17:05.457929Z","iopub.execute_input":"2025-07-20T07:17:05.458182Z","iopub.status.idle":"2025-07-20T07:17:05.471523Z","shell.execute_reply.started":"2025-07-20T07:17:05.458163Z","shell.execute_reply":"2025-07-20T07:17:05.471002Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# =====================\n# Model Wrapper\n# =====================\nclass CLIPClassifier(nn.Module):\n    def __init__(self, clip_model, num_classes=3):\n        super().__init__()\n        self.clip = clip_model\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 2, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n        self.loss_fn = FocalLoss(weight=class_weights.to(device))\n\n    def forward(self, input_ids, attention_mask, pixel_values, labels=None):\n        outputs = self.clip(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n        text_emb = outputs.text_embeds\n        image_emb = outputs.image_embeds\n        fused = torch.cat([text_emb, image_emb], dim=1)\n        logits = self.classifier(fused)\n        if labels is not None:\n            loss = self.loss_fn(logits, labels)\n            return loss, logits\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:17:05.472297Z","iopub.execute_input":"2025-07-20T07:17:05.472619Z","iopub.status.idle":"2025-07-20T07:17:05.483147Z","shell.execute_reply.started":"2025-07-20T07:17:05.472596Z","shell.execute_reply":"2025-07-20T07:17:05.482688Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# =====================\n# Load Data\n# =====================\nval_dataset = MemeDataset(df_val, transform=val_transform)\ntest_dataset = MemeDataset(df_test, transform=val_transform, is_train=False)\n\n# Weighted Sampling\nclass_counts = df_train['label'].value_counts().sort_index().values\nsampling_weights = 1. / class_counts\nsample_weights = df_train['label'].apply(lambda x: sampling_weights[x])\nsampler = WeightedRandomSampler(sample_weights.tolist(), len(sample_weights), replacement=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n\nmodel = CLIPClassifier(clip_model).to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\ntotal_steps = len(train_loader) * 10\nscheduler = get_cosine_schedule_with_warmup(optimizer, int(0.1 * total_steps), total_steps)\n\nscaler = torch.cuda.amp.GradScaler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:17:05.483899Z","iopub.execute_input":"2025-07-20T07:17:05.484139Z","iopub.status.idle":"2025-07-20T07:17:05.769510Z","shell.execute_reply.started":"2025-07-20T07:17:05.484118Z","shell.execute_reply":"2025-07-20T07:17:05.768757Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2938823221.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# =====================\n# Training and Eval\n# =====================\ndef train_one_epoch(model, loader):\n    model.train()\n    total_loss, preds, labels = 0, [], []\n    for batch in tqdm(loader, desc=\"Training\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.cuda.amp.autocast():\n            loss, logits = model(**batch)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        scheduler.step()\n        total_loss += loss.item()\n        preds += logits.argmax(1).detach().cpu().tolist()\n        labels += batch[\"labels\"].cpu().tolist()\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average=\"macro\")\n    return total_loss / len(loader), acc, f1\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, preds, labels = 0, [], []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Evaluating\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            loss, logits = model(**batch)\n            total_loss += loss.item()\n            preds += logits.argmax(1).cpu().tolist()\n            labels += batch[\"labels\"].cpu().tolist()\n    print(classification_report(labels, preds, target_names=LABEL_MAP.keys()))\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average=\"macro\")\n    return total_loss / len(loader), acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:17:05.770305Z","iopub.execute_input":"2025-07-20T07:17:05.770491Z","iopub.status.idle":"2025-07-20T07:17:05.777859Z","shell.execute_reply.started":"2025-07-20T07:17:05.770470Z","shell.execute_reply":"2025-07-20T07:17:05.777314Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# =====================\n# Training Loop\n# =====================\nbest_f1 = 0\npatience = 3\ncounter = 0\nfor epoch in range(10):\n    print(f\"\\nEpoch {epoch + 1}\")\n    train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader)\n    val_loss, val_acc, val_f1 = evaluate(model, val_loader)\n    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n    print(f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        counter = 0\n        torch.save(model.state_dict(), \"best_clip_model.pt\")\n        print(\"✅ Saved best model\")\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"⏹️ Early stopping\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:17:05.778401Z","iopub.execute_input":"2025-07-20T07:17:05.778608Z","iopub.status.idle":"2025-07-20T07:17:06.479512Z","shell.execute_reply.started":"2025-07-20T07:17:05.778593Z","shell.execute_reply":"2025-07-20T07:17:06.478426Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/254 [00:00<?, ?it/s]/tmp/ipykernel_36/3950364737.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nTraining:   0%|          | 0/254 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2976132820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3950364737.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1663704140.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, pixel_values, labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtext_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mimage_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         )\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         text_outputs: BaseModelOutputWithPooling = self.text_model(\n\u001b[0m\u001b[1;32m   1044\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# CLIP's text model uses causal mask, prepare it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_position_embedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;34mf\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;34mf\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Sequence length must be less than max_position_embeddings (got `sequence length`: 128 and max_position_embeddings: 77"],"ename":"ValueError","evalue":"Sequence length must be less than max_position_embeddings (got `sequence length`: 128 and max_position_embeddings: 77","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"# =====================\n# Inference\n# =====================\ndef predict_and_export(model, loader, output_file=\"submission.json\"):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Predicting\"):\n            indices = batch.pop(\"index\")\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(**batch)\n            if isinstance(logits, tuple): logits = logits[1]\n            preds = torch.argmax(logits, dim=1).cpu().tolist()\n            for idx, label in zip(indices, preds):\n                predictions.append({\"index\": idx, \"prediction\": INVERSE_LABEL_MAP[label]})\n    with open(output_file, \"w\") as f:\n        json.dump(predictions, f, indent=2)\n    print(f\"✅ Predictions saved to {output_file}\")\n\nmodel.load_state_dict(torch.load(\"best_clip_model.pt\"))\npredict_and_export(model, test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:17:06.479890Z","iopub.status.idle":"2025-07-20T07:17:06.480101Z","shell.execute_reply.started":"2025-07-20T07:17:06.480001Z","shell.execute_reply":"2025-07-20T07:17:06.480010Z"}},"outputs":[],"execution_count":null}]}